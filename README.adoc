= Pero-B35

image::pero.png[Pero from Metropolis]

A lightweight CLI for comfortable command-line chat with LLMs. Named after Pero, the robot detective from Rintaro's _Metropolis_ (2001). Thin wrapper around OpenAI-compatible APIs with tool calling, multimodal input, and MCP support.

== Commands

=== `pero chat`

Process a text buffer as a conversation. The key idea: you work on a text buffer (a file, a Vim buffer, a TiddlyWiki tiddler) and pipe it through `pero chat`. The output includes the full conversation plus the new assistant response, so you can keep building on it over time.

Simplest usage - just ask a question:

```
What is 2+2?
```

The response comes back with role markers, ready for follow-up:

```
What is 2+2?

A>>
2+2 equals 4.
```

Add `Q>>` for your next question, pipe again, repeat:

```
What is 2+2?

A>>
2+2 equals 4.

Q>>
And 3+3?
```

Role markers: `Q>>` user, `A>>` assistant, `S>>` system (optional, for custom instructions).

```sh
# from stdin
cat conversation.txt | pero chat

# from file
pero chat -f conversation.txt

# with tools enabled
pero chat --default-tools -f task.txt

# output only (just the assistant response, no conversation)
pero chat -o -f conversation.txt
```

=== `pero serve`

HTTP server exposing the same functionality. Useful for editor integrations or other tools.

```sh
pero serve                     # localhost:3535
pero serve -H 0.0.0.0 -P 8080  # custom host/port
```

POST plain text to `/`, get plain text back. Query params override defaults:

```sh
curl -X POST http://localhost:3535/?model=gpt-4o \
  -H "Content-Type: text/plain" \
  -d "Q>>\nHello"
```

== Configuration

Create a `.env` file in the project root with API keys for your chosen gateway(s).

=== Supported Gateways

[cols="1,2,2", options="header"]
|===
| Gateway | Environment Variable | Notes

| `openrouter` (default)
| `OPENROUTER_API_KEY`
| Access to 100+ models via https://openrouter.ai[OpenRouter]

| `openai`
| `OPENAI_API_KEY`
| Direct OpenAI API access

| `anthropic`
| `ANTHROPIC_API_KEY`
| Claude models via https://anthropic.com[Anthropic]

| `gemini`
| `GEMINI_API_KEY`
| Google's Gemini models (uses OpenAI-compatible endpoint)

| `deepseek`
| `DEEPSEEK_API_KEY`
| https://deepseek.com[DeepSeek] models

| `ollama`
| _(none)_
| Local models via http://127.0.0.1:11434[Ollama] (no API key needed)
|===

Example `.env` file:

```
OPENROUTER_API_KEY=sk-or-v1-...
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=AI...
DEEPSEEK_API_KEY=sk-...
```

Use `-g <gateway>` to select a gateway:

```sh
pero chat -g anthropic -m claude-sonnet-4-20250514
pero chat -g gemini -m gemini-2.0-flash
pero chat -g ollama -m llama3
pero chat -g openrouter -m anthropic/claude-sonnet-4.5  # default
```

=== `chat` Options

```
pero chat

Options:
      --help                  Show help                                [boolean]
      --version               Show version number                      [boolean]
  -f, --file                  file to read from                         [string]
  -m, --model                 model to use                              [string]
  -g, --gateway               gateway provider                          [string]
  -t, --tools                 tools config file(s)                       [array]
      --default-tools         include auto-discovered tools           [boolean]
  -p, --preamble              offline files to prepend to prompt         [array]
  -o, --output-only           output only, no chat formatting         [boolean]
  -r, --include-reasoning     include reasoning tokens if available   [boolean]
  -R, --reasoning-effort      low|medium|high (reasoning models)        [string]
      --include-tool          none|call|result (include @@.tools block) [string]
      --tools-placement       top|bottom (placement of @@.tools block)  [string]
      --max-tokens            maximum tokens to generate                [number]
```

=== Tools configuration

Tools are disabled by default. Enable them with:

- `--default-tools` - Load auto-discovered tools from:
  1) ./tools.yaml in current working directory
  2) ./tools.yml in current working directory  
  3) bundled default tools.yaml
- `-t, --tools <file>` - Load specific tool configuration file(s)

Combine both flags to load auto-discovered and custom tools together.

==== Bash tools

Define shell commands as tools in YAML:

```yaml
read_file:
  description: Read the contents of a file
  parameters:
    file_path: The path to the file to read
  command: cat "{{file_path}}"

search_web:
  description: Search the web
  parameters:
    query: The search query
  command: ddgr -C -x --np -n 25 "{{query}}"
```

==== MCP servers

Connect to https://modelcontextprotocol.io[Model Context Protocol] servers by adding `_mcp_servers` to your tools.yaml:

```yaml
_mcp_servers:
  time:
    command: uvx
    args: [mcp-server-time, --local-timezone=Europe/Warsaw]
  
  filesystem:
    command: npx
    args: [-y, "@modelcontextprotocol/server-filesystem", "/home/user/docs"]
```

MCP servers are connected on startup, their tools are discovered automatically and merged with any bash tools. Environment variables are inherited from the parent process.

=== Content interpolation

User messages support special tags for including external content:

==== Text files

```
[txt[path/to/file.txt]]
```

Includes the file content wrapped in `<FILE path="...">` tags.

==== Images

```
[img[path/to/image.jpg]]
[img[https://example.com/image.png]]
```

Local images are base64-encoded; remote URLs are passed directly.

==== Audio

```
[audio[path/to/recording.wav]]
```

Supported formats: `.wav`, `.mp3`, `.m4a`, `.ogg`, `.flac`, `.aac`, `.webm`

Audio handling varies by gateway (OpenAI uses `input_audio`, Gemini uses `inline_data`).
